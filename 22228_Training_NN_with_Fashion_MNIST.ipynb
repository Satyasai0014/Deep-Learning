{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PZHAI4xNGO8o"
      },
      "outputs": [],
      "source": [
        "# Importing required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torchvision import datasets,transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module): # Constructing Neural Network\n",
        "    def __init__(self):\n",
        "        super(SimpleNN,self).__init__()\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.linear_relu_stack=nn.Sequential(\n",
        "            nn.Linear(28*28,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,11)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "VsyEerYFG2N0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpleNN = SimpleNN()\n",
        "print(simpleNN) # printing the network"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxPC_INjGnCb",
        "outputId": "a31ccd86-5256-4bc8-e218-19b49c4f50c6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleNN(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=64, out_features=11, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in simpleNN.parameters(): # Random initialization of parameters\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfZX__LfGm_T",
        "outputId": "aaad1b6a-a52a-49c0-a390-94883ffdb285"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0258,  0.0332,  0.0072,  ...,  0.0034,  0.0199,  0.0011],\n",
            "        [-0.0179, -0.0169, -0.0147,  ..., -0.0286,  0.0110,  0.0319],\n",
            "        [-0.0205,  0.0159, -0.0068,  ...,  0.0250, -0.0241, -0.0169],\n",
            "        ...,\n",
            "        [ 0.0312,  0.0244,  0.0278,  ...,  0.0079, -0.0198,  0.0056],\n",
            "        [-0.0012,  0.0080,  0.0023,  ...,  0.0041, -0.0071,  0.0029],\n",
            "        [ 0.0066, -0.0066, -0.0311,  ...,  0.0304,  0.0252,  0.0101]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0130,  0.0334,  0.0308, -0.0314,  0.0097, -0.0017, -0.0251, -0.0133,\n",
            "        -0.0020, -0.0265, -0.0029, -0.0044,  0.0152, -0.0300, -0.0017,  0.0313,\n",
            "         0.0225,  0.0253, -0.0107,  0.0353, -0.0183,  0.0142, -0.0123, -0.0123,\n",
            "         0.0024, -0.0329, -0.0101, -0.0352,  0.0153, -0.0007,  0.0236, -0.0300,\n",
            "         0.0316,  0.0130, -0.0119, -0.0088, -0.0041,  0.0042, -0.0088,  0.0085,\n",
            "         0.0299,  0.0296,  0.0020,  0.0114, -0.0101,  0.0058, -0.0038, -0.0171,\n",
            "         0.0135, -0.0092,  0.0171,  0.0157, -0.0255, -0.0354,  0.0059, -0.0258,\n",
            "         0.0163, -0.0295, -0.0289, -0.0210, -0.0158, -0.0224,  0.0016, -0.0238,\n",
            "        -0.0135, -0.0351, -0.0129, -0.0187,  0.0345,  0.0190,  0.0215,  0.0023,\n",
            "        -0.0048, -0.0160,  0.0289, -0.0258, -0.0293, -0.0297, -0.0229, -0.0228,\n",
            "        -0.0330,  0.0152,  0.0087, -0.0260,  0.0023,  0.0173, -0.0008,  0.0214,\n",
            "        -0.0166, -0.0111,  0.0326, -0.0025, -0.0190,  0.0056,  0.0150, -0.0247,\n",
            "         0.0261,  0.0088,  0.0162, -0.0351, -0.0255,  0.0266,  0.0095,  0.0051,\n",
            "         0.0338,  0.0058, -0.0089,  0.0023, -0.0315,  0.0004, -0.0101,  0.0196,\n",
            "        -0.0260,  0.0304, -0.0108, -0.0013,  0.0218,  0.0329,  0.0005, -0.0128,\n",
            "         0.0177, -0.0042, -0.0148, -0.0246, -0.0026, -0.0038, -0.0194,  0.0157,\n",
            "         0.0027,  0.0141, -0.0006, -0.0130, -0.0059,  0.0247,  0.0214, -0.0169,\n",
            "         0.0088, -0.0222, -0.0156, -0.0124, -0.0103, -0.0350,  0.0350, -0.0203,\n",
            "         0.0119, -0.0255, -0.0025,  0.0264,  0.0238,  0.0175, -0.0129,  0.0134,\n",
            "         0.0287, -0.0228, -0.0296,  0.0309, -0.0141, -0.0046,  0.0058,  0.0317,\n",
            "        -0.0212,  0.0246, -0.0004, -0.0129,  0.0253,  0.0302,  0.0001, -0.0350,\n",
            "        -0.0197, -0.0326,  0.0048, -0.0050, -0.0133,  0.0005, -0.0142, -0.0244,\n",
            "         0.0105,  0.0162,  0.0058, -0.0102,  0.0038, -0.0294, -0.0216, -0.0160,\n",
            "        -0.0336, -0.0021, -0.0107,  0.0237,  0.0198, -0.0092,  0.0257, -0.0251,\n",
            "         0.0151, -0.0314,  0.0027, -0.0164,  0.0130, -0.0039,  0.0037,  0.0187,\n",
            "         0.0009,  0.0056, -0.0073,  0.0083, -0.0356, -0.0323,  0.0095, -0.0250,\n",
            "         0.0221,  0.0281,  0.0003, -0.0285, -0.0328,  0.0123,  0.0327,  0.0201,\n",
            "         0.0050,  0.0246,  0.0250,  0.0329,  0.0312,  0.0066,  0.0095,  0.0056,\n",
            "        -0.0233,  0.0258, -0.0156,  0.0290, -0.0260, -0.0038,  0.0150, -0.0035,\n",
            "         0.0104, -0.0210,  0.0338,  0.0179,  0.0230, -0.0052,  0.0292,  0.0144,\n",
            "         0.0208, -0.0170,  0.0063, -0.0052, -0.0249,  0.0267,  0.0049,  0.0162,\n",
            "        -0.0142, -0.0042,  0.0118,  0.0188,  0.0077, -0.0017,  0.0096,  0.0031,\n",
            "         0.0142,  0.0082, -0.0201,  0.0048,  0.0285,  0.0259, -0.0192,  0.0035,\n",
            "        -0.0354, -0.0285, -0.0027,  0.0204,  0.0090,  0.0120,  0.0120,  0.0192,\n",
            "        -0.0201,  0.0025, -0.0138,  0.0062, -0.0306, -0.0155, -0.0341, -0.0221,\n",
            "         0.0282, -0.0209, -0.0235, -0.0247, -0.0030, -0.0351, -0.0311, -0.0075,\n",
            "         0.0073,  0.0292,  0.0310,  0.0264, -0.0230,  0.0352,  0.0203, -0.0101,\n",
            "        -0.0020, -0.0059, -0.0340,  0.0033, -0.0195, -0.0132,  0.0155,  0.0316,\n",
            "         0.0262, -0.0220, -0.0298,  0.0246, -0.0066,  0.0304,  0.0258, -0.0124,\n",
            "        -0.0196,  0.0266, -0.0214,  0.0127, -0.0174,  0.0307,  0.0026, -0.0284,\n",
            "         0.0242,  0.0116,  0.0249,  0.0167, -0.0345,  0.0076, -0.0113, -0.0036,\n",
            "         0.0241,  0.0316,  0.0295,  0.0285, -0.0098, -0.0172, -0.0181,  0.0058,\n",
            "        -0.0198,  0.0248,  0.0094, -0.0296,  0.0272,  0.0255,  0.0251, -0.0312,\n",
            "        -0.0305,  0.0249,  0.0131,  0.0092,  0.0308, -0.0317, -0.0163,  0.0323,\n",
            "         0.0343, -0.0077,  0.0346,  0.0015,  0.0146, -0.0004,  0.0224,  0.0248,\n",
            "         0.0345, -0.0329, -0.0346,  0.0067,  0.0352,  0.0280,  0.0002,  0.0091,\n",
            "         0.0349, -0.0205,  0.0242,  0.0218, -0.0297, -0.0194,  0.0291, -0.0328,\n",
            "         0.0289, -0.0060,  0.0174,  0.0193,  0.0073, -0.0248,  0.0341,  0.0176,\n",
            "        -0.0151, -0.0333,  0.0077, -0.0298, -0.0045,  0.0215,  0.0047, -0.0317,\n",
            "        -0.0150, -0.0055,  0.0340, -0.0287, -0.0066,  0.0089, -0.0343, -0.0030,\n",
            "         0.0199, -0.0138,  0.0171, -0.0045, -0.0258, -0.0251, -0.0118, -0.0082,\n",
            "        -0.0123,  0.0153, -0.0309,  0.0007, -0.0138, -0.0328, -0.0112, -0.0258,\n",
            "         0.0144, -0.0209,  0.0017,  0.0052,  0.0271, -0.0063,  0.0163, -0.0127,\n",
            "         0.0312, -0.0111,  0.0149,  0.0229, -0.0189, -0.0069,  0.0052,  0.0203,\n",
            "         0.0312,  0.0109, -0.0233, -0.0257,  0.0083,  0.0261,  0.0137, -0.0182,\n",
            "        -0.0125,  0.0147,  0.0295, -0.0025,  0.0291, -0.0009, -0.0270, -0.0117,\n",
            "         0.0014, -0.0038, -0.0160,  0.0041, -0.0101,  0.0348, -0.0255,  0.0096,\n",
            "         0.0038,  0.0147,  0.0037, -0.0116, -0.0132,  0.0332, -0.0124, -0.0062,\n",
            "         0.0313, -0.0053,  0.0135,  0.0253,  0.0057,  0.0126, -0.0197,  0.0282,\n",
            "         0.0048, -0.0164,  0.0071, -0.0140,  0.0047,  0.0259, -0.0102, -0.0124,\n",
            "         0.0091,  0.0268, -0.0164, -0.0342, -0.0298,  0.0304, -0.0087, -0.0099,\n",
            "        -0.0205,  0.0057, -0.0249, -0.0172, -0.0187, -0.0321, -0.0271, -0.0295,\n",
            "        -0.0013,  0.0346,  0.0066, -0.0138, -0.0325, -0.0291,  0.0226, -0.0336,\n",
            "        -0.0236, -0.0066,  0.0160,  0.0140,  0.0102,  0.0130, -0.0152, -0.0127],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0381,  0.0298, -0.0410,  ...,  0.0079, -0.0428, -0.0381],\n",
            "        [ 0.0259, -0.0004, -0.0438,  ..., -0.0357,  0.0388, -0.0136],\n",
            "        [-0.0123, -0.0315, -0.0124,  ...,  0.0221,  0.0271,  0.0367],\n",
            "        ...,\n",
            "        [ 0.0193, -0.0146,  0.0228,  ..., -0.0304,  0.0194,  0.0378],\n",
            "        [ 0.0251,  0.0028, -0.0318,  ...,  0.0072, -0.0059,  0.0074],\n",
            "        [-0.0128, -0.0091, -0.0349,  ...,  0.0360, -0.0353,  0.0021]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0410,  0.0200, -0.0021, -0.0160, -0.0417, -0.0337,  0.0197, -0.0017,\n",
            "         0.0055, -0.0279,  0.0262,  0.0409, -0.0209,  0.0385, -0.0310,  0.0273,\n",
            "        -0.0183,  0.0151, -0.0325,  0.0288,  0.0430, -0.0140, -0.0401, -0.0251,\n",
            "        -0.0269, -0.0046,  0.0327,  0.0287, -0.0054, -0.0181, -0.0118, -0.0258,\n",
            "         0.0241, -0.0036,  0.0337, -0.0376,  0.0249, -0.0249,  0.0310, -0.0366,\n",
            "         0.0213,  0.0210, -0.0320, -0.0257, -0.0336, -0.0381,  0.0222,  0.0414,\n",
            "         0.0360, -0.0433,  0.0162, -0.0061,  0.0352, -0.0278,  0.0051,  0.0390,\n",
            "        -0.0243, -0.0018, -0.0307,  0.0266,  0.0317, -0.0252,  0.0144, -0.0024,\n",
            "        -0.0061, -0.0376,  0.0211,  0.0097, -0.0095,  0.0069, -0.0154,  0.0234,\n",
            "        -0.0028,  0.0320, -0.0008, -0.0145,  0.0357,  0.0330,  0.0355,  0.0145,\n",
            "        -0.0045, -0.0008, -0.0219, -0.0043,  0.0262, -0.0332, -0.0334,  0.0268,\n",
            "         0.0201,  0.0168,  0.0043, -0.0053, -0.0420, -0.0028,  0.0077, -0.0195,\n",
            "        -0.0333,  0.0079, -0.0265, -0.0215, -0.0218,  0.0205,  0.0095,  0.0390,\n",
            "         0.0182,  0.0291,  0.0213, -0.0404,  0.0350,  0.0080,  0.0085,  0.0147,\n",
            "         0.0260,  0.0359, -0.0315,  0.0384,  0.0327, -0.0370, -0.0109, -0.0325,\n",
            "         0.0124,  0.0314, -0.0119,  0.0023,  0.0328, -0.0195,  0.0435, -0.0027],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0801, -0.0425,  0.0270,  ...,  0.0346, -0.0498, -0.0527],\n",
            "        [ 0.0807, -0.0380, -0.0028,  ...,  0.0615,  0.0521, -0.0455],\n",
            "        [-0.0169, -0.0034,  0.0504,  ...,  0.0727, -0.0138, -0.0024],\n",
            "        ...,\n",
            "        [ 0.0195,  0.0287,  0.0199,  ...,  0.0544, -0.0279,  0.0082],\n",
            "        [ 0.0770, -0.0016, -0.0211,  ..., -0.0794,  0.0718, -0.0426],\n",
            "        [ 0.0552, -0.0768,  0.0687,  ...,  0.0471, -0.0282,  0.0441]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0272,  0.0476,  0.0221, -0.0113, -0.0783,  0.0521,  0.0299,  0.0303,\n",
            "        -0.0808, -0.0051, -0.0734, -0.0611, -0.0419,  0.0076,  0.0039, -0.0307,\n",
            "        -0.0504,  0.0476,  0.0430,  0.0715, -0.0320,  0.0326, -0.0758, -0.0079,\n",
            "        -0.0395, -0.0159,  0.0791,  0.0386, -0.0534, -0.0210, -0.0609,  0.0547,\n",
            "        -0.0275,  0.0534,  0.0015,  0.0458, -0.0748,  0.0658,  0.0809, -0.0118,\n",
            "         0.0498, -0.0082,  0.0226, -0.0047,  0.0405, -0.0005,  0.0666,  0.0307,\n",
            "         0.0508, -0.0253,  0.0753,  0.0376, -0.0655, -0.0472, -0.0146, -0.0414,\n",
            "         0.0367, -0.0360,  0.0727,  0.0870,  0.0408,  0.0585,  0.0661, -0.0380],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.1117,  0.1049, -0.0839, -0.1128, -0.0346, -0.1065, -0.0227, -0.0832,\n",
            "          0.0175,  0.0901,  0.0376, -0.0876,  0.0193, -0.0641,  0.0854,  0.0658,\n",
            "          0.0144,  0.0706,  0.0924,  0.0948,  0.0719,  0.1228,  0.0066, -0.0925,\n",
            "         -0.0395,  0.0807, -0.0717,  0.1207, -0.0567, -0.0083,  0.1157,  0.0690,\n",
            "          0.0009, -0.0015,  0.0248,  0.0331,  0.1092, -0.0461,  0.0553,  0.0866,\n",
            "         -0.0950, -0.0493,  0.0599,  0.1007,  0.0785,  0.0777,  0.0884, -0.0346,\n",
            "         -0.1031,  0.0327, -0.0395,  0.1230, -0.0928,  0.0104,  0.0130,  0.0352,\n",
            "          0.1065, -0.1069, -0.0205, -0.0558, -0.0269, -0.0851,  0.1043,  0.0231],\n",
            "        [-0.0475,  0.1009, -0.0031, -0.1002,  0.0076, -0.0122,  0.0754,  0.1167,\n",
            "         -0.0924, -0.1050,  0.0177,  0.1126,  0.1115,  0.0554,  0.0792,  0.0696,\n",
            "         -0.0005,  0.0600, -0.0375,  0.0972, -0.1060, -0.0588,  0.0365, -0.0848,\n",
            "          0.0884,  0.0058,  0.0405, -0.0228,  0.0007,  0.0047,  0.0690,  0.0023,\n",
            "         -0.0437, -0.0483,  0.1205,  0.0504,  0.0004, -0.0234, -0.0136, -0.0636,\n",
            "         -0.0627, -0.1137, -0.0968,  0.0062, -0.1229, -0.0499, -0.0469,  0.0959,\n",
            "         -0.0812, -0.0621,  0.0674,  0.0698,  0.0272, -0.0164,  0.0086, -0.0782,\n",
            "         -0.0583, -0.0548,  0.0697,  0.1154,  0.0344, -0.1233, -0.0548,  0.0566],\n",
            "        [-0.0914,  0.0556, -0.0767,  0.0968, -0.0193,  0.0267,  0.0748,  0.0559,\n",
            "         -0.1184, -0.0993, -0.0045,  0.0851,  0.0386, -0.0753, -0.1237,  0.0465,\n",
            "          0.0855,  0.0177, -0.0524, -0.0429, -0.0243, -0.0029, -0.1248, -0.0949,\n",
            "         -0.0510,  0.0012, -0.1142,  0.0957, -0.1249,  0.0752,  0.0157, -0.0790,\n",
            "          0.0970,  0.0794,  0.0291,  0.0604, -0.0457, -0.0221,  0.0392,  0.0208,\n",
            "         -0.0830,  0.0957, -0.0706,  0.0462,  0.0471, -0.0893,  0.0252,  0.0713,\n",
            "         -0.0587,  0.0846, -0.0587, -0.0220, -0.0429,  0.0107, -0.0919,  0.1035,\n",
            "         -0.0899,  0.0729,  0.0241, -0.0960, -0.0280, -0.0409, -0.0751, -0.0796],\n",
            "        [ 0.1077, -0.0601, -0.0056, -0.0122,  0.0990,  0.1217, -0.0815,  0.0472,\n",
            "          0.1141, -0.0606,  0.0687,  0.0869,  0.0383,  0.0980, -0.0404, -0.0796,\n",
            "          0.0998,  0.0040, -0.0147,  0.0771, -0.0414, -0.0183, -0.0664,  0.0424,\n",
            "         -0.0733, -0.0015, -0.1081, -0.0275,  0.0258, -0.0668, -0.0776,  0.1043,\n",
            "          0.0928, -0.0092,  0.0781,  0.0111,  0.1070,  0.0384, -0.0960,  0.0182,\n",
            "          0.1184,  0.1086,  0.0069,  0.1041,  0.0772,  0.1011,  0.0939,  0.0712,\n",
            "          0.0424, -0.1080,  0.1231,  0.0662,  0.0750, -0.0558,  0.0796,  0.1186,\n",
            "          0.1110,  0.0064,  0.0214,  0.0854,  0.0924, -0.0315,  0.0556, -0.0856],\n",
            "        [-0.1207, -0.0271, -0.0114,  0.0389, -0.1163, -0.0431, -0.0853, -0.0802,\n",
            "         -0.0104,  0.0768,  0.0805, -0.0106, -0.0750,  0.0709,  0.0280,  0.0736,\n",
            "         -0.0091, -0.0612, -0.1143, -0.1048,  0.0630,  0.0559,  0.1134,  0.1205,\n",
            "         -0.0568,  0.0503, -0.1152, -0.0875,  0.0692, -0.0364,  0.0615, -0.0820,\n",
            "         -0.1155, -0.1043, -0.0619,  0.0203,  0.0745, -0.0167, -0.0323, -0.0125,\n",
            "          0.0375, -0.0627,  0.0302, -0.0697, -0.0365,  0.0055, -0.1174, -0.0249,\n",
            "          0.0742,  0.0097, -0.0634, -0.0758,  0.0307,  0.0322, -0.0890, -0.0515,\n",
            "         -0.0974,  0.0252,  0.0543, -0.1093,  0.0976, -0.0619, -0.0616, -0.1247],\n",
            "        [ 0.0426, -0.0895, -0.0862, -0.0873,  0.0759, -0.0375,  0.0094,  0.1050,\n",
            "          0.0752,  0.1145, -0.0078,  0.0171, -0.0658,  0.1080,  0.0795, -0.0197,\n",
            "         -0.0530,  0.0365, -0.0765,  0.0546, -0.0569, -0.0233,  0.0256,  0.0515,\n",
            "         -0.0257, -0.0988,  0.0075,  0.1194,  0.0400, -0.1117, -0.0451, -0.1192,\n",
            "         -0.0062, -0.0390,  0.0786, -0.1186,  0.0845,  0.0052, -0.0460,  0.0572,\n",
            "         -0.1164,  0.1169,  0.0804,  0.1139, -0.0229, -0.0402, -0.0846, -0.1009,\n",
            "         -0.0451, -0.0634, -0.0127, -0.0707,  0.1029,  0.0711, -0.0052,  0.0773,\n",
            "          0.0115,  0.0789,  0.1050,  0.0421, -0.0117,  0.0775,  0.0565, -0.1113],\n",
            "        [-0.0631, -0.0647,  0.0146, -0.0714, -0.0946, -0.0608, -0.0580,  0.1114,\n",
            "          0.0954, -0.1166, -0.0145,  0.0901,  0.0682,  0.0325,  0.0027, -0.0532,\n",
            "          0.0780, -0.0213,  0.0433,  0.0446,  0.0714,  0.0246, -0.0748, -0.0606,\n",
            "         -0.1079,  0.0477, -0.0353,  0.1071, -0.0324,  0.1133, -0.0484, -0.0132,\n",
            "         -0.0387,  0.0932, -0.0028,  0.0289, -0.0500,  0.0780,  0.1204, -0.1218,\n",
            "          0.1207, -0.0449,  0.0945,  0.0469,  0.1035, -0.0358,  0.0798, -0.0978,\n",
            "         -0.0013,  0.0247, -0.0190, -0.0631,  0.0777,  0.0563, -0.0131, -0.0886,\n",
            "          0.0914, -0.0876,  0.0177,  0.0221, -0.0185, -0.0882, -0.1015, -0.0938],\n",
            "        [ 0.0797,  0.0874, -0.1077,  0.0335, -0.0511,  0.1068, -0.0251, -0.0069,\n",
            "          0.0943,  0.0749, -0.0593, -0.0212,  0.0368, -0.0719, -0.0533,  0.0852,\n",
            "         -0.0254,  0.0764,  0.0211, -0.0319,  0.0287, -0.0662, -0.0572, -0.0191,\n",
            "         -0.0060,  0.0801,  0.0617,  0.1120,  0.0745,  0.0143,  0.0555, -0.1139,\n",
            "          0.0466,  0.0708,  0.0099,  0.0382, -0.0989,  0.1191,  0.1199,  0.0113,\n",
            "          0.0522, -0.0115,  0.0902,  0.0389, -0.0470,  0.0407,  0.0212, -0.1022,\n",
            "          0.1208,  0.0523, -0.0191,  0.1082,  0.1227,  0.0926, -0.0294,  0.0934,\n",
            "         -0.1063,  0.0843, -0.0679,  0.0980, -0.0666,  0.1095, -0.0379,  0.0389],\n",
            "        [ 0.0911,  0.0350,  0.0164, -0.0782, -0.0006,  0.1060,  0.0811, -0.0613,\n",
            "          0.0128,  0.0200,  0.1168,  0.0952, -0.0976,  0.0068, -0.0845, -0.0389,\n",
            "          0.1241, -0.0934,  0.0097, -0.0782,  0.0255,  0.0392, -0.0624,  0.0605,\n",
            "          0.0865,  0.0691, -0.1181, -0.0957,  0.0792,  0.0647,  0.0895, -0.0479,\n",
            "         -0.0699,  0.0853,  0.0204,  0.0146, -0.1130,  0.0322,  0.0988,  0.0384,\n",
            "         -0.0251, -0.0524, -0.0716, -0.0333, -0.1151, -0.0223, -0.0839, -0.0560,\n",
            "         -0.0752,  0.1153,  0.1187,  0.0554,  0.0817,  0.0452,  0.1102,  0.0602,\n",
            "         -0.0849, -0.0854,  0.1186, -0.0046, -0.0369,  0.1092, -0.0898, -0.1137],\n",
            "        [ 0.1242, -0.0578, -0.1042, -0.0331,  0.0986,  0.0398,  0.0375,  0.0555,\n",
            "         -0.0241,  0.0678, -0.0219,  0.0292,  0.0786, -0.0727, -0.0591,  0.0554,\n",
            "          0.1239, -0.0733, -0.1070,  0.0590,  0.0620, -0.1214, -0.0203,  0.0847,\n",
            "         -0.0708, -0.0274, -0.0731, -0.0155,  0.0110, -0.0264, -0.0717, -0.0013,\n",
            "         -0.1008,  0.0990, -0.0196,  0.0081,  0.0244, -0.0090, -0.0939, -0.0513,\n",
            "         -0.0023,  0.0935, -0.0165, -0.0904,  0.0301,  0.1013, -0.0800, -0.1117,\n",
            "         -0.0261,  0.0259, -0.0491,  0.1054,  0.1190,  0.0195,  0.0448, -0.0532,\n",
            "          0.0195,  0.0357,  0.0037, -0.0460, -0.1012, -0.0768,  0.1080,  0.0069],\n",
            "        [-0.0255, -0.0306, -0.0768, -0.0667,  0.0036,  0.0275, -0.0407,  0.0004,\n",
            "          0.0658,  0.0259,  0.1081,  0.0643,  0.0216, -0.0507,  0.1224,  0.1115,\n",
            "          0.0844,  0.0939,  0.0553,  0.0867,  0.1098,  0.0907, -0.0077, -0.0670,\n",
            "         -0.0013,  0.0231, -0.1108,  0.0282, -0.0482,  0.1243,  0.1182, -0.0873,\n",
            "         -0.0483,  0.1164,  0.0313,  0.0250, -0.1033, -0.0523,  0.0016, -0.1146,\n",
            "          0.0200,  0.0429, -0.0577, -0.0106, -0.0948, -0.1086,  0.0095, -0.0104,\n",
            "          0.0749, -0.0272, -0.0235, -0.0192,  0.0165,  0.0934, -0.0572,  0.0525,\n",
            "         -0.0484,  0.0892, -0.1033, -0.0382, -0.1007,  0.0378,  0.1144,  0.0985]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0502,  0.0004, -0.0645, -0.0183,  0.0771, -0.1178, -0.0178, -0.0671,\n",
            "        -0.1026, -0.0018, -0.0726], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "test_data=datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "LuWj0SNjGm8k"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=DataLoader(test_data,batch_size=64,shuffle=True)"
      ],
      "metadata": {
        "id": "eZeKbNLHGm5b"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_size=0.1\n",
        "data_size=len(training_data)\n",
        "val_size=int(v_size*data_size)\n",
        "train_size=data_size-val_size"
      ],
      "metadata": {
        "id": "M8zjsYsSGmwc"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset,validation_dataset=random_split(training_data,[train_size,val_size])\n",
        "train_data=DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
        "val_data=DataLoader(validation_dataset,batch_size=64,shuffle=True)"
      ],
      "metadata": {
        "id": "EhYQ4FLWHxIC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_epochs = 10\n",
        "learning_rate = 0.0001"
      ],
      "metadata": {
        "id": "xwV7gL-GHxFi"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=nn.CrossEntropyLoss() # Cross Entropy loss is used"
      ],
      "metadata": {
        "id": "llUWiW8HHxC7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=optim.SGD(simpleNN.parameters(),lr=learning_rate,momentum=0.9) # Stochastic Gradient Descent optimizer is uesd"
      ],
      "metadata": {
        "id": "XV1Pmkf9HxAO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader,model,loss_fn,optimizer):\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()# setting the model to training mode\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "        pred=model(X)# computing the prediction and loss\n",
        "        loss=loss_fn(pred,y)\n",
        "        loss.backward()#performing backward propagataion\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch%100==0:\n",
        "            loss,current=loss.item(),(batch+1)*len(X)\n",
        "            print(f\"Loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader,model,loss_fn):\n",
        "    model.eval()#setting the model to evaluation mode\n",
        "    size=len(dataloader.dataset)\n",
        "    num_batches=len(dataloader)\n",
        "    test_loss,correct=0,0\n",
        "    with torch.no_grad():# to not calculate the gradients during test mode\n",
        "        for X,y in dataloader:\n",
        "            pred=model(X)\n",
        "            test_loss+=loss_fn(pred,y).item()\n",
        "            correct+=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "        test_loss/=num_batches\n",
        "        correct/=size\n",
        "        print(f\"validation error: \\n validation loss: {test_loss:>8f}\\n\")"
      ],
      "metadata": {
        "id": "fhXD3ndwHw9l"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(num_epochs): # Training\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------------------\")\n",
        "    train_loop(train_data,simpleNN,loss,optimizer)\n",
        "    test_loop(val_data,simpleNN,loss)\n",
        "print(\"Training Completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL72dCstHw6j",
        "outputId": "5a560990-f0f3-4d65-bf2c-15902c66255a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------------------\n",
            "Loss: 2.382809 [   64/54000]\n",
            "Loss: 2.381959 [ 6464/54000]\n",
            "Loss: 2.394509 [12864/54000]\n",
            "Loss: 2.369274 [19264/54000]\n",
            "Loss: 2.369697 [25664/54000]\n",
            "Loss: 2.373699 [32064/54000]\n",
            "Loss: 2.372780 [38464/54000]\n",
            "Loss: 2.354548 [44864/54000]\n",
            "Loss: 2.353386 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 2.357427\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------------------\n",
            "Loss: 2.351303 [   64/54000]\n",
            "Loss: 2.357157 [ 6464/54000]\n",
            "Loss: 2.343284 [12864/54000]\n",
            "Loss: 2.341186 [19264/54000]\n",
            "Loss: 2.350852 [25664/54000]\n",
            "Loss: 2.338837 [32064/54000]\n",
            "Loss: 2.324759 [38464/54000]\n",
            "Loss: 2.316993 [44864/54000]\n",
            "Loss: 2.331938 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 2.318626\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------------------\n",
            "Loss: 2.331049 [   64/54000]\n",
            "Loss: 2.314676 [ 6464/54000]\n",
            "Loss: 2.297890 [12864/54000]\n",
            "Loss: 2.301160 [19264/54000]\n",
            "Loss: 2.282994 [25664/54000]\n",
            "Loss: 2.276294 [32064/54000]\n",
            "Loss: 2.284391 [38464/54000]\n",
            "Loss: 2.251574 [44864/54000]\n",
            "Loss: 2.247283 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 2.254275\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------------------\n",
            "Loss: 2.272604 [   64/54000]\n",
            "Loss: 2.265886 [ 6464/54000]\n",
            "Loss: 2.245030 [12864/54000]\n",
            "Loss: 2.221890 [19264/54000]\n",
            "Loss: 2.196702 [25664/54000]\n",
            "Loss: 2.185469 [32064/54000]\n",
            "Loss: 2.183484 [38464/54000]\n",
            "Loss: 2.131893 [44864/54000]\n",
            "Loss: 2.163269 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 2.128112\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------------------\n",
            "Loss: 2.120842 [   64/54000]\n",
            "Loss: 2.107362 [ 6464/54000]\n",
            "Loss: 2.111954 [12864/54000]\n",
            "Loss: 2.054493 [19264/54000]\n",
            "Loss: 1.977592 [25664/54000]\n",
            "Loss: 1.976265 [32064/54000]\n",
            "Loss: 1.979140 [38464/54000]\n",
            "Loss: 1.939641 [44864/54000]\n",
            "Loss: 1.968280 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 1.887270\n",
            "\n",
            "Epoch 6\n",
            "-------------------------------------------\n",
            "Loss: 1.920680 [   64/54000]\n",
            "Loss: 1.793690 [ 6464/54000]\n",
            "Loss: 1.838553 [12864/54000]\n",
            "Loss: 1.781888 [19264/54000]\n",
            "Loss: 1.735489 [25664/54000]\n",
            "Loss: 1.759724 [32064/54000]\n",
            "Loss: 1.652232 [38464/54000]\n",
            "Loss: 1.677607 [44864/54000]\n",
            "Loss: 1.658229 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 1.603951\n",
            "\n",
            "Epoch 7\n",
            "-------------------------------------------\n",
            "Loss: 1.555751 [   64/54000]\n",
            "Loss: 1.572661 [ 6464/54000]\n",
            "Loss: 1.514202 [12864/54000]\n",
            "Loss: 1.497501 [19264/54000]\n",
            "Loss: 1.510761 [25664/54000]\n",
            "Loss: 1.508313 [32064/54000]\n",
            "Loss: 1.486038 [38464/54000]\n",
            "Loss: 1.429292 [44864/54000]\n",
            "Loss: 1.330213 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 1.400586\n",
            "\n",
            "Epoch 8\n",
            "-------------------------------------------\n",
            "Loss: 1.376121 [   64/54000]\n",
            "Loss: 1.407476 [ 6464/54000]\n",
            "Loss: 1.404809 [12864/54000]\n",
            "Loss: 1.401648 [19264/54000]\n",
            "Loss: 1.355328 [25664/54000]\n",
            "Loss: 1.326705 [32064/54000]\n",
            "Loss: 1.438315 [38464/54000]\n",
            "Loss: 1.218860 [44864/54000]\n",
            "Loss: 1.246707 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 1.259205\n",
            "\n",
            "Epoch 9\n",
            "-------------------------------------------\n",
            "Loss: 1.236021 [   64/54000]\n",
            "Loss: 1.225076 [ 6464/54000]\n",
            "Loss: 1.189319 [12864/54000]\n",
            "Loss: 1.160386 [19264/54000]\n",
            "Loss: 1.138960 [25664/54000]\n",
            "Loss: 1.109806 [32064/54000]\n",
            "Loss: 1.117419 [38464/54000]\n",
            "Loss: 1.169727 [44864/54000]\n",
            "Loss: 1.290364 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 1.149841\n",
            "\n",
            "Epoch 10\n",
            "-------------------------------------------\n",
            "Loss: 1.106946 [   64/54000]\n",
            "Loss: 1.130558 [ 6464/54000]\n",
            "Loss: 1.053953 [12864/54000]\n",
            "Loss: 1.020460 [19264/54000]\n",
            "Loss: 1.170634 [25664/54000]\n",
            "Loss: 1.109441 [32064/54000]\n",
            "Loss: 1.083326 [38464/54000]\n",
            "Loss: 1.005665 [44864/54000]\n",
            "Loss: 1.041007 [51264/54000]\n",
            "validation error: \n",
            " validation loss: 1.059932\n",
            "\n",
            "Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(simpleNN,'simpleNN.pth') # Saving the model\n"
      ],
      "metadata": {
        "id": "dCqKyFIRK-NR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=torch.load('simpleNN.pth') # Loading the model"
      ],
      "metadata": {
        "id": "UM04eHZoK-Ki"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dcHiVZ6eK-Ha"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpleNN.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFwGGrdgMQNS",
        "outputId": "f21c7067-9e68-412e-9bdb-cbbf2eb771e2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleNN(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=64, out_features=11, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map=[\n",
        "    \"T-Shirt\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle Boot\"\n",
        "]"
      ],
      "metadata": {
        "id": "a3MhrHkmMSPC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for batch in test_data:\n",
        "    inputs, _ = batch  # Assuming the DataLoader returns inputs and labels, but we're only interested in inputs\n",
        "    with torch.no_grad():\n",
        "        outputs = simpleNN(inputs)  # Forward pass to get predictions\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    _, predicted_classes = torch.max(probabilities, 1)\n",
        "    predictions.extend(predicted_classes.tolist())"
      ],
      "metadata": {
        "id": "mJaVp2GnMXcy"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (inputs, labels) in enumerate(test_data):\n",
        "    if batch_idx >= 2:\n",
        "        break  # Only process the first two batches\n",
        "\n",
        "    # Forward pass to get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = simpleNN(inputs)\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    _, predicted_classes = torch.max(probabilities, 1)\n",
        "\n",
        "    # Convert the predicted class index to class label\n",
        "    predicted_label = labels_map[predicted_classes[0].item()]\n",
        "\n",
        "    # Display the image and its corresponding predicted label\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(inputs[0].permute(1, 2, 0))\n",
        "    plt.title(f'Predicted Label: {predicted_label}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "QsRNdG8VMXZ6",
        "outputId": "c7c335b5-aed0-40b6-a115-ee7d4fece02d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFeCAYAAADnm4a1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZEElEQVR4nO3de3CV9b3v8c/Kyv0CRAiYCEaMYiiVspvWo1iFVsA2qLM7o2yd2oLuauTgBVs2to5DRXCopy1iuYndrfRwOFoYa/VYLIUKp0C7latbUC5lg1ZwcxESLrmv9dt/MKxNDF/5PsACYd6vGWZk5ZMnv3XJJ88Kz9dfLIQQBABoJ+NsLwAAPqsoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoyHPQJZdcohEjRqT+vnTpUsViMS1duvSsremTPrnGM2HgwIH6/Oc/f1qPeTbuBz47KMiIZs+erVgslvqTm5urXr166f7779euXbvO9vIiWbBggR5//PGzuoZYLKb777//rK4h3Xbt2qUxY8aosrJS+fn5KigoUFVVlSZOnKja2tq0fd3PwvN7rss82ws4Vz3xxBPq2bOnGhsbtXz5cs2cOVMLFizQ+vXrlZ+ff0bXcv3116uhoUHZ2dmRPm/BggWaPn0630RptHLlSlVXV+vQoUO68847VVVVJUlatWqVfvzjH+vPf/6z/vjHP6bla/P8njoK8iR94xvf0Je+9CVJ0ne/+1117txZkydP1iuvvKI77rjjuJ9z+PBhFRQUnPa1ZGRkKDc397QfF6emtrZW3/zmNxWPx7V27VpVVla2+fiTTz6pX/ziF2dpdfDgLfZp8rWvfU2StG3bNknSiBEjVFhYqK1bt6q6ulpFRUX61re+JUlKJpOaMmWK+vTpo9zcXHXr1k01NTXav39/m2OGEDRx4kR1795d+fn5+upXv6oNGza0+9rW7yDffPNNVVdXq7i4WAUFBerbt6+eeeaZ1PqmT58uSW1+ZXDU6V7jqXjllVc0dOhQlZWVKScnRxUVFZowYYISicRx86tXr1b//v2Vl5ennj176tlnn22XaWpq0o9+9CNddtllysnJUY8ePTR27Fg1NTWdcD1bt27V1q1bT5ibNWuWduzYocmTJ7crR0nq1q2bHnvssTa3zZgxQ3369FFOTo7Kyso0atSodm/Dly1bpttuu00XX3xxau0PP/ywGhoaUpkTPb/w4QzyNDn6DdO5c+fUba2trbrxxhv1la98RT/96U9Tb71ramo0e/Zs3XXXXXrwwQe1bds2TZs2TWvXrtWKFSuUlZUlSRo3bpwmTpyo6upqVVdXa82aNRoyZIiam5tPuJ5FixbppptuUmlpqR566CFdeOGFeu+99/Taa6/poYceUk1NjXbu3KlFixZpzpw57T7/TKzRa/bs2SosLNT3vvc9FRYW6o033tC4ceN04MAB/eQnP2mT3b9/v6qrqzVs2DDdcccdmjdvnkaOHKns7Gzdfffdko6U/y233KLly5fr3nvvVe/evfXOO+/o6aef1ubNm/W73/3uU9dzww03SJK2b9/+qblXX31VeXl5uvXWW1338/HHH9f48eM1aNAgjRw5Ups2bdLMmTO1cuXKNo/5/PnzVV9fr5EjR6pz58566623NHXqVH344YeaP3++JJ3w+YVTQCTPP/98kBQWL14c9uzZE/7+97+HF198MXTu3Dnk5eWFDz/8MIQQwvDhw4Ok8IMf/KDN5y9btixICnPnzm1z+x/+8Ic2t+/evTtkZ2eHoUOHhmQymco9+uijQVIYPnx46rYlS5YESWHJkiUhhBBaW1tDz549Q3l5edi/f3+br3PssUaNGhWO9xJIxxotksKoUaM+NVNfX9/utpqampCfnx8aGxtTtw0YMCBICj/72c9StzU1NYV+/fqFrl27hubm5hBCCHPmzAkZGRlh2bJlbY757LPPBklhxYoVqdvKy8vb3Y/y8vJQXl5+wvtWXFwcvvCFL5wwF8J/P5ZDhgwJiUQidfu0adOCpPCrX/0qddvxHo9JkyaFWCwW3n///dRt1vMLP95in6RBgwappKREPXr00O23367CwkK9/PLLuuiii9rkRo4c2ebv8+fPV8eOHTV48GDt3bs39aeqqkqFhYVasmSJJGnx4sVqbm7WAw880Oat0ejRo0+4trVr12rbtm0aPXq0OnXq1OZjnrdZZ2KNUeTl5aX+++DBg9q7d6+uu+461dfXa+PGjW2ymZmZqqmpSf09OztbNTU12r17t1avXp26f71791ZlZWWb+3f01yRH759l+/btJzx7lKQDBw6oqKjIdR+PPpajR49WRsZ/f1vec8896tChg37/+9+nbjv28Th8+LD27t2r/v37K4SgtWvXur4efHiLfZKmT5+uXr16KTMzU926ddMVV1zR5oUtHflm7d69e5vbtmzZorq6OnXt2vW4x929e7ck6f3335ckXX755W0+XlJSouLi4k9d29G3+yd7TeCZWGMUGzZs0GOPPaY33nhDBw4caPOxurq6Nn8vKytr9w9hvXr1knSk2K6++mpt2bJF7733nkpKSo779Y7ev1PVoUMHHTx40JU9+lheccUVbW7Pzs7WpZdemvq4JH3wwQcaN26cXn311Xa/E/7k44FTQ0GepKuuuir1r9iWnJycdqWZTCbVtWtXzZ0797ifY33TnkmfpTXW1tZqwIAB6tChg5544glVVFQoNzdXa9as0SOPPKJkMhn5mMlkUldeeaUmT5583I/36NHjVJctSaqsrNS6devU3Nwc+RIsSyKR0ODBg7Vv3z498sgjqqysVEFBgXbs2KERI0ac1OMBGwV5hlVUVGjx4sW69tpr27xV+qTy8nJJR87mLr300tTte/bsaXfWcLyvIUnr16/XoEGDzJz1dvtMrNFr6dKl+vjjj/Xb3/5W119/fer2o1cLfNLOnTvbXU61efNmSUemYqQj9+/tt9/WDTfckNZ/2b355pv117/+VS+99JJ56ddRRx/LTZs2tXksm5ubtW3bttTz+M4772jz5s369a9/re985zup3KJFi9odk3+1PnX8DvIMGzZsmBKJhCZMmNDuY62tralLOgYNGqSsrCxNnTpV4Zh91aZMmXLCr/HFL35RPXv21JQpU9pdInLssY6WyCczZ2KNXvF4vN26m5ubNWPGjOPmW1tbNWvWrDbZWbNmqaSkJHWR9rBhw7Rjx47jXoPY0NCgw4cPf+qavJf53HfffSotLdX3v//9VEkfa/fu3Zo4caKkI49ldna2fv7zn7e5r7/85S9VV1enoUOHSjr+4xFCSF2+dSzr+YUfZ5Bn2IABA1RTU6NJkyZp3bp1GjJkiLKysrRlyxbNnz9fzzzzjG699VaVlJRozJgxmjRpkm666SZVV1dr7dq1ev3119WlS5dP/RoZGRmaOXOmbr75ZvXr10933XWXSktLtXHjRm3YsEELFy6UpFRhPPjgg7rxxhsVj8d1++23n5E1HmvVqlWpojjWwIED1b9/fxUXF2v48OF68MEHFYvFNGfOnDYFcayysjI99dRT2r59u3r16qXf/OY3WrdunZ577rnUZTLf/va3NW/ePN13331asmSJrr32WiUSCW3cuFHz5s3TwoULP/XXJ97LfIqLi/Xyyy+rurpa/fr1azNJs2bNGr3wwgu65pprJB35tcUPf/hDjR8/Xl//+td1yy23aNOmTZoxY4a+/OUv684775R05G17RUWFxowZox07dqhDhw566aWXjnvGbj2/iOCs/fv5OeroZT4rV6781Nzw4cNDQUGB+fHnnnsuVFVVhby8vFBUVBSuvPLKMHbs2LBz585UJpFIhPHjx4fS0tKQl5cXBg4cGNavX9/u0pNPXuZz1PLly8PgwYNDUVFRKCgoCH379g1Tp05Nfby1tTU88MADoaSkJMRisXaXhJzONVokmX8mTJgQQghhxYoV4eqrrw55eXmhrKwsjB07NixcuLDdfR4wYEDo06dPWLVqVbjmmmtCbm5uKC8vD9OmTWv3dZubm8NTTz0V+vTpE3JyckJxcXGoqqoK48ePD3V1dancqVzmc9TOnTvDww8/HHr16hVyc3NDfn5+qKqqCk8++WSbrxXCkct6KisrQ1ZWVujWrVsYOXJku0u13n333TBo0KBQWFgYunTpEu65557w9ttvB0nh+eefT+VO9PzixGIhsC82ABwPv4MEAAMFCQAGChIADBQkABgoSAAwUJAAYKAgAcDgnqQZnHFbOteBCD76fn93tqGb/zLXi5a2urMF7/o3KGvd/oE7G0U8wv80Ixb3nwvs+1pPd3bXjf7/MXDxihx3tstzf3VnEd2i5HxXjjNIADBQkABgoCABwEBBAoCBggQAAwUJAAYKEgAMFCQAGChIADBQkABgYNOuNNr6k2vc2TFDX3VnL8g8/n7Vx/N/P7ranb180G539lvF/+bO9svxj9hV/Okud/a+f/izOzvjza+6s2UX+R+Hb5Zsd2f7X/M3d3bT/aXu7NwXbnBnu0/6izsLziABwERBAoCBggQAAwUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoAhFkJwbXvHroZHNPzjVe7srClT3NmFhz/nzraEuDu7v6XAnX1xuX80suPFde7swS2d3NmKMf4Rxo/v8a+3+53/4c6+X1vszv7jJf/uzkbRPXufO9sv179r5KP/9M/+Rbz1jj97jmFXQwA4RRQkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABjY1TCiDwfF3Nm6pH83v2Tw/6zKz2h2Z7vl+UcCf1H9r+5s54x6d/ZfCm91Z7f/pq87W1y0x5199OLX3Nk9FxW5s9ubS9zZKCOi9RFeO1GO+5/9/fftwrfc0fMWZ5AAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYKAgAcDAqGFEsU7+Mb/cWKs7m5/R5M7WJfLd2aZklju7q7WjO1uU0ejODijZ4s4uj1W4s+WF/p3//qO5qzu7qbHUnc3NaHFno4yIXpB5yJ1tDP7n+ODlCXf2Qnfy/MUZJAAYKEgAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMDBqGNHQyvXu7IHg35kuyshaUdw/YlebrrHEFv9YYpT71pL079BXku0fx9vZUuzORlGcedid7RT3Z/e1FrqzjXH/8/alL/zNnfXvh3n+4gwSAAwUJAAYKEgAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYGDWMaGSX/+/Obmrx76SXFfPvNpcIMXc2Gfw/A+uT2e5slPHBKG4rW+3ORtkJ8mAyz51tjDBymRvz71QYRVHcv2vkxwn/WOL/LF3izk5SX3f2fMUZJAAYKEgAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMDBqKCkj37/z32VZ/p0K32n279B3UeZ+d3Zd48XubKd4vTt7QaZ/l8CsWKs7mx1hjDJDybQcNzfpH40siDDC+FGE3RJLs/zPcUX2bnd2XWO5Ozs4/wN3Npbpr4fQ6n89nEs4gwQAAwUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAZGDSU1DOgTIb3MnYwruLPX5fpHtRrDR+7s1ub07KyYDLnubJQdEKPsKBhFS0jPS71L5kF3tjLH/7xVZfvHVLc0+0cj/9+hCne26YZ+7mz2wlXu7LmEM0gAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGBg1FBS7eX+8bbf13d0Zy/MrHVnH9p5jTt7VdFWdzaKukSeO5sM/p+t+1rT83M4HvPvgJiIsN6kYu7svZ3+5s72eeEBdzbroH+9r939v9zZFQ2XuLP7eme7sxcudEfPKZxBAoCBggQAAwUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAA6OGkpo6pee45Zn17uyqZ/7BnT0wKsedHVP6R3f29YNXurM5EXYqjDKWmBFhfDBddjd3cGdzYv4x1R6L/DtX5q/f6c5W1BS6sysa3FE1XOjflfN8xRkkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwMGooqaXIP1KVIf8oXPdM/whYx//zb+7su3n+HRB7P77EnX2x1b+rYdds/6hhFFHGEqOIMsKYiHDekAj+42YvXOXO+ocSo4nH/K/1RPfGNK3i3MEZJAAYKEgAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMDBqKCmZ5x8XK8hoSuNKfDpsa3ZnMxRLyxriirDj3Wdgp8Io683P8D++kdbQ5wp3NrFhU1rWUJTh39awYwf/rpznK84gAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYKAgAcBAQQKAgVFDSbkXHvZnY1F280vPz59t/+Q/bqsS7mxWhj97rklEGLlsSvq/LZIRRhg3/TDfnb3sTndULcH/vEV5/XbvWOfOnv0B3PTgDBIADBQkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgYNZRU2XWXO9sSojxk6dnN7ztX/cWdXdUUd2ej7OYXZXTvXFMY9w/O/anBPz44t/+/urM/UlVa1hBlV8NLCj52Z9OzB+PZxxkkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwMGooqXt+bVqOu7nFv1tiFMM6rnJnVzf2cGdzMqLs2OiXDP6fwxmx9IxnRllDfoRRw3cbL3JnHyr+mzsbxYLavu7s3Z2Xu7NRRi7P13Ot8/NeAcBpQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGCgIAHAQEECgIFRQ0l5cf+I3VU5je7s/z7Q62SWc0KVWTnu7MsHurizHeP+He8iSdP4YLrEFdzZ/Yk8/3Fj/vOReJfO7uzqvR3d2adLs9zZOcmzv4Pn2cYZJAAYKEgAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMDBqKCk/o9mdzYn5H7I3D1zqzsb7lPmzsXXubH0i252NMmoYZffBrM/CGFrMH61P+kc5ExF2S0wE/+NQ/2X/a+c/97a6s1HGHXMy/Mc9X8+1zs97BQCnAQUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAZGDdPocKt/zG/HYP8udi0hcTLLOaEo44NRJNL0czgeYYQxK+Z/zKJkmyLt/OdXe5l/98HkoQhzlIiEM0gAMFCQAGCgIAHAQEECgIGCBAADBQkABgoSAAwUJAAYKEgAMFCQAGBg1FDSgdbc9By32X/chq4hLWuIsjNdlBG7lhA/meWcE/IzmtzZ0uy6tKyhpcifjR9Kz3nOoYR/d0epJS1rONs4gwQAAwUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAZGDZW+Xfd2Hyp0Z1sL/Tv0/akh353NzfCPgB1KpGfkMspuicngfy6iHLclpOelHmXk8k8N/tG9pgv89y27Nj2v371N/tevtD8tazjbOIMEAAMFCQAGChIADBQkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGRg0l7Wv2j+5tbPHveNfYnOXO9vzcR+7s/8jxj3VlKMLIWoRdDaNIKObORtlZMV3iER6zKD6X1ejOll25y52te73Und2dOOzOdspucGc/difPLZxBAoCBggQAAwUJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAA6OGkjbv7+rOJkr9Y3O9Sva4sw0D/KNl1z0yxp1tuiC4s60l/h0Q4zlpGks8FOElGfzPhWL+xyGz1r+GwvcjrCHC6UjXaX9xZ/eM848aRnGgJcoul/4xynMJZ5AAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYKAgAcDAqKGkgw057mxc/pG1hlb/roZRXPSUfwwN57/WQv9rMora5rwo6bSs4WzjDBIADBQkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgYNZSUm+3fzS835t/Nb299vjvbxZ2UYlnZEdJ+oaU5Lcf9TIj5dx+MxeMRjus/x4hl+b/dkvX17myI+0cNo5wRZcaS7qz/O+jcwhkkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwMGoo6eChKLu3+e3bW+TORhk1PK9HAtMl+MfxQmtrepaQ8I+pRjpuhMnIxgiPw8EW/26f52uRcAYJAAYKEgAMFCQAGChIADBQkABgoCABwEBBAoCBggQAAwUJAAYKEgAM5+uEUNpUZBW6s5f22JOeRUTYoS/KiB3SLPh3CYwis9S/A2L3TP/rt2N2gzt72J08t3AGCQAGChIADBQkABgoSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADIwaSiqd69+9rWfdve5s76f3ubOR9rtjfPDclKbnrWSef1fOvjl3uLMZbxS7s92UprHas4wzSAAwUJAAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYIiFwNwaABwPZ5AAYKAgAcBAQQKAgYIEAAMFCQAGChIADBQkABgoSAAwUJAAYPgvVzTYOorSiDEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}